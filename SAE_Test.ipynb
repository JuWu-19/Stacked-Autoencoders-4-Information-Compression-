{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my74KfRCk70Y",
        "outputId": "c9825d3e-969f-4a25-ae78-abb607112e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load data"
      ],
      "metadata": {
        "id": "CXjmUZYArVAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "\n",
        "data_list = []\n",
        "\n",
        "main_folder_path = '/content/drive/MyDrive/fSAE/'\n",
        "sub_folders = ['1', '2', '3', '4']\n",
        "\n",
        "folders = [os.path.join(main_folder_path, sub_folder) for sub_folder in sub_folders]\n",
        "\n",
        "\n",
        "for folder in folders:\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith('.mat'):\n",
        "            mat_data = scipy.io.loadmat(os.path.join(folder, file))\n",
        "            # Assuming the key in the .mat file is 'data_key'\n",
        "            data_vector = mat_data['Y']\n",
        "            # Flatten the vector before appending\n",
        "            data_list.append(data_vector.ravel())\n",
        "# Convert list to numpy array\n",
        "data_array = np.array(data_list)\n"
      ],
      "metadata": {
        "id": "U24Uz4pxp4DK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T7SMVTb0Gvp",
        "outputId": "4663657a-f45c-4850-9cfb-997481ce184e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.io\n",
        "\n",
        "# Path to one of your .mat files for inspection\n",
        "sample_mat_file_path = os.path.join('/content/drive/MyDrive/fSAE/1', '1.mat')  # Replace 'sample_file.mat' with the name of one of your .mat files\n",
        "\n",
        "# Load the .mat file\n",
        "mat_data = scipy.io.loadmat(sample_mat_file_path)\n",
        "\n",
        "# Print the keys\n",
        "print(mat_data.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQG206ymzsAE",
        "outputId": "01c82a5b-1589-4f7e-c561-595c102603af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'Y'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model definition, set the training configuration and train model, model compilation, Model Evaluation and visualization"
      ],
      "metadata": {
        "id": "M46WIRjpwmYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "input_dim = 10000\n",
        "encoding_dim = 200  # Compressed representation\n",
        "\n",
        "# Model Architecture with He Initialization\n",
        "input_data = Input(shape=(input_dim,))\n",
        "\n",
        "# Encoding layers with LeakyReLU and BatchNormalization\n",
        "encoded = Dense(8000, kernel_initializer='he_normal')(input_data)\n",
        "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "\n",
        "encoded = Dense(4000, kernel_initializer='he_normal')(encoded)\n",
        "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "\n",
        "encoded = Dense(1000, kernel_initializer='he_normal')(encoded)\n",
        "encoded = LeakyReLU(alpha=0.01)(encoded)\n",
        "encoded = BatchNormalization()(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "\n",
        "encoded = Dense(encoding_dim, kernel_initializer='he_normal')(encoded)\n",
        "encoded2 = LeakyReLU(alpha=0.01)(encoded)\n",
        "\n",
        "# Decoding layers with LeakyReLU and BatchNormalization\n",
        "decoded = Dense(1000, kernel_initializer='he_normal')(encoded2)\n",
        "decoded = LeakyReLU(alpha=0.01)(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "\n",
        "decoded = Dense(4000, kernel_initializer='he_normal')(decoded)\n",
        "decoded = LeakyReLU(alpha=0.01)(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "\n",
        "decoded = Dense(8000, kernel_initializer='he_normal')(decoded)\n",
        "decoded = LeakyReLU(alpha=0.01)(decoded)\n",
        "decoded = BatchNormalization()(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "\n",
        "decoded = Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder2 = Model(inputs=input_data, outputs=decoded)"
      ],
      "metadata": {
        "id": "a6AOeTDdSlHB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import he_normal\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "optimizer2 = Adam(learning_rate=1e-3)  # You can adjust the learning rate here\n",
        "autoencoder2.compile(optimizer=optimizer2, loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "nQ2WZ33CTII-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.1,\n",
        "                              patience=10,\n",
        "                              verbose=1,\n",
        "                              min_delta=1e-4,\n",
        "                              mode='min')"
      ],
      "metadata": {
        "id": "dHa9tErJfVXo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle and split data sets"
      ],
      "metadata": {
        "id": "_G72wL-Qwfxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data_array)\n",
        "\n",
        "# Add small Gaussian noise\n",
        "noise_factor = 0.0005  # Adjust this value based on your requirements\n",
        "data_noisy = data_scaled + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data_scaled.shape)\n",
        "\n",
        "# Ensure the noisy data is still between 0 and 1\n",
        "data_noisy = np.clip(data_noisy, 0., 1.)\n",
        "\n",
        "# Split both the noisy data and the original scaled data\n",
        "x_train_noisy, x_val_noisy, x_train, x_val = train_test_split(data_noisy, data_scaled, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "eIhOeGWGwiL5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2= autoencoder2.fit(x_train_noisy, x_train,\n",
        "                          epochs=100,\n",
        "                          batch_size=256,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_val_noisy, x_val),\n",
        "                          callbacks=[reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xDfwPaxTT8J",
        "outputId": "749c8d90-6293-45cd-ffef-4041469c60d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 45s 9s/step - loss: 1.7946 - val_loss: 99.1633 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 1.6558 - val_loss: 1045.8032 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 41s 11s/step - loss: 1.5373 - val_loss: 3304.5527 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 37s 9s/step - loss: 1.4308 - val_loss: 6466.3926 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 38s 9s/step - loss: 1.3441 - val_loss: 8807.3447 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 1.2555 - val_loss: 10387.6191 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 1.1619 - val_loss: 11735.4023 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 42s 10s/step - loss: 1.0941 - val_loss: 9561.3906 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 45s 11s/step - loss: 1.0192 - val_loss: 9124.5283 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 39s 9s/step - loss: 0.9917 - val_loss: 4428.1484 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.9320 \n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.9320 - val_loss: 3274.4846 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 38s 9s/step - loss: 0.8840 - val_loss: 387.1938 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.7022 - val_loss: 278.7401 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.7533 - val_loss: 171.7444 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.7023 - val_loss: 51.3768 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6871 - val_loss: 45.4555 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6819 - val_loss: 18.8213 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6658 - val_loss: 15.9801 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6625 - val_loss: 10.5290 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6556 - val_loss: 6.6252 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6490 - val_loss: 5.3700 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.6438 - val_loss: 4.1668 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6401 - val_loss: 3.2612 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.6361 - val_loss: 2.5855 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6316 - val_loss: 2.1565 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6296 - val_loss: 1.6902 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6259 - val_loss: 1.3222 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.6211 - val_loss: 1.1702 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6158 - val_loss: 0.9702 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6135 - val_loss: 0.8465 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6090 - val_loss: 0.6908 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.6069 - val_loss: 0.6916 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.6053 - val_loss: 0.4909 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5998 - val_loss: 0.4647 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5999 - val_loss: 0.3993 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.5934 - val_loss: 0.3315 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.5898 - val_loss: 0.2737 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5866 - val_loss: 0.2457 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5850 - val_loss: 0.2576 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5811 - val_loss: 0.2192 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 41s 11s/step - loss: 0.5791 - val_loss: 0.1927 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.5769 - val_loss: 0.1862 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5743 - val_loss: 0.1578 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5706 - val_loss: 0.1450 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.5668 - val_loss: 0.1252 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.5631 - val_loss: 0.1205 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5594 - val_loss: 0.1060 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5587 - val_loss: 0.0957 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.5553 - val_loss: 0.0939 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.5530 - val_loss: 0.0856 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5484 - val_loss: 0.0778 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5485 - val_loss: 0.0777 - lr: 1.0000e-04\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5472 - val_loss: 0.0757 - lr: 1.0000e-04\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.5415 - val_loss: 0.0683 - lr: 1.0000e-04\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5423 - val_loss: 0.0649 - lr: 1.0000e-04\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5359 - val_loss: 0.0639 - lr: 1.0000e-04\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.5335 - val_loss: 0.0602 - lr: 1.0000e-04\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.5330 - val_loss: 0.0586 - lr: 1.0000e-04\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 39s 9s/step - loss: 0.5292 - val_loss: 0.0549 - lr: 1.0000e-04\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5273 - val_loss: 0.0516 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.5245 - val_loss: 0.0501 - lr: 1.0000e-04\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5227 - val_loss: 0.0495 - lr: 1.0000e-04\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.5195 - val_loss: 0.0466 - lr: 1.0000e-04\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 42s 10s/step - loss: 0.5168 - val_loss: 0.0455 - lr: 1.0000e-04\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 39s 9s/step - loss: 0.5131 - val_loss: 0.0445 - lr: 1.0000e-04\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5143 - val_loss: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.5089 - val_loss: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5079 - val_loss: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5016 - val_loss: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.5027 - val_loss: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.4999 - val_loss: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4981 - val_loss: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4961 - val_loss: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4945 - val_loss: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.4921 - val_loss: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4901 - val_loss: 0.0350 - lr: 1.0000e-04\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4872 - val_loss: 0.0351 - lr: 1.0000e-04\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.4848 - val_loss: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.4813 - val_loss: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4795 - val_loss: 0.0324 - lr: 1.0000e-04\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4770 - val_loss: 0.0313 - lr: 1.0000e-04\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.4741 - val_loss: 0.0322 - lr: 1.0000e-04\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.4738 - val_loss: 0.0315 - lr: 1.0000e-04\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.4724 - val_loss: 0.0310 - lr: 1.0000e-04\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4690 - val_loss: 0.0308 - lr: 1.0000e-04\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4675 - val_loss: 0.0311 - lr: 1.0000e-04\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.4655 - val_loss: 0.0310 - lr: 1.0000e-04\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4657 - val_loss: 0.0314 - lr: 1.0000e-04\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 39s 9s/step - loss: 0.4608 - val_loss: 0.0304 - lr: 1.0000e-04\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4594 - val_loss: 0.0295 - lr: 1.0000e-04\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4562 - val_loss: 0.0291 - lr: 1.0000e-04\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 41s 11s/step - loss: 0.4526 - val_loss: 0.0281 - lr: 1.0000e-04\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 42s 11s/step - loss: 0.4504 - val_loss: 0.0286 - lr: 1.0000e-04\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 42s 11s/step - loss: 0.4482 - val_loss: 0.0277 - lr: 1.0000e-04\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 39s 10s/step - loss: 0.4477 - val_loss: 0.0280 - lr: 1.0000e-04\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4442 - val_loss: 0.0278 - lr: 1.0000e-04\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4422 - val_loss: 0.0278 - lr: 1.0000e-04\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 41s 10s/step - loss: 0.4421 - val_loss: 0.0284 - lr: 1.0000e-04\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 38s 10s/step - loss: 0.4403 - val_loss: 0.0288 - lr: 1.0000e-04\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 40s 10s/step - loss: 0.4381 - val_loss: 0.0287 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2_continue= autoencoder2.fit(x_train_noisy, x_train,\n",
        "                          epochs=100,\n",
        "                          batch_size=256,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(x_val_noisy, x_val),\n",
        "                          callbacks=[reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc8m-bAGS3vU",
        "outputId": "66de7005-3cb1-4fbf-f9eb-6d547055161e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 48s 10s/step - loss: 0.4250 - val_loss: 0.0284 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 42s 10s/step - loss: 0.4213 - val_loss: 0.0280 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.4208 - val_loss: 0.0276 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 42s 10s/step - loss: 0.4169 - val_loss: 0.0277 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4159 - val_loss: 0.0281 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4147 - val_loss: 0.0281 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 42s 10s/step - loss: 0.4134 - val_loss: 0.0278 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.4105 - val_loss: 0.0279 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4094 - val_loss: 0.0278 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4069 - val_loss: 0.0274 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 45s 11s/step - loss: 0.4050 - val_loss: 0.0272 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4026 - val_loss: 0.0274 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4010 - val_loss: 0.0268 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.4005 - val_loss: 0.0273 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.3959 - val_loss: 0.0273 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3944 - val_loss: 0.0269 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3945 - val_loss: 0.0272 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3913 - val_loss: 0.0270 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3902 - val_loss: 0.0272 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3865 - val_loss: 0.0272 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3868 - val_loss: 0.0271 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3848 - val_loss: 0.0274 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3821 \n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3821 - val_loss: 0.0271 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3801 - val_loss: 0.0246 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3747 - val_loss: 0.0202 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3694 - val_loss: 0.0177 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3680 - val_loss: 0.0173 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3669 - val_loss: 0.0177 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3671 - val_loss: 0.0179 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3670 - val_loss: 0.0176 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3668 - val_loss: 0.0172 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3677 - val_loss: 0.0170 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3660 - val_loss: 0.0169 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 45s 11s/step - loss: 0.3658 - val_loss: 0.0169 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3649 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3660 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3646 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3646 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3644 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3640 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3642 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3648 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3636 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3642 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3626 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3632 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3633 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3631 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3632 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3632 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3635 - val_loss: 0.0168 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3615 \n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3615 - val_loss: 0.0167 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3610 - val_loss: 0.0167 - lr: 1.0000e-06\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3613 - val_loss: 0.0166 - lr: 1.0000e-06\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3626 - val_loss: 0.0165 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3620 - val_loss: 0.0164 - lr: 1.0000e-06\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3611 - val_loss: 0.0163 - lr: 1.0000e-06\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3610 - val_loss: 0.0162 - lr: 1.0000e-06\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3607 - val_loss: 0.0161 - lr: 1.0000e-06\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3611 - val_loss: 0.0160 - lr: 1.0000e-06\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3595 - val_loss: 0.0160 - lr: 1.0000e-06\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3606 - val_loss: 0.0159 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3597 - val_loss: 0.0159 - lr: 1.0000e-06\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3603 - val_loss: 0.0158 - lr: 1.0000e-06\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3608 - val_loss: 0.0158 - lr: 1.0000e-06\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3593 - val_loss: 0.0158 - lr: 1.0000e-06\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3603 - val_loss: 0.0158 - lr: 1.0000e-06\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3607 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3599 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3606 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3592 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3607 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3602 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3603 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3601 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3601 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3601 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 43s 11s/step - loss: 0.3591 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3607 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3596 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3596 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3598 \n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3598 - val_loss: 0.0157 - lr: 1.0000e-06\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3605 - val_loss: 0.0157 - lr: 1.0000e-07\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3601 - val_loss: 0.0157 - lr: 1.0000e-07\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3598 - val_loss: 0.0157 - lr: 1.0000e-07\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3607 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3593 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3601 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3589 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 46s 11s/step - loss: 0.3600 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3597 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.3594 \n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3594 - val_loss: 0.0156 - lr: 1.0000e-07\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 46s 11s/step - loss: 0.3590 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3591 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3605 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3598 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3616 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 46s 11s/step - loss: 0.3613 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 44s 11s/step - loss: 0.3603 - val_loss: 0.0156 - lr: 1.0000e-08\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 43s 10s/step - loss: 0.3594 - val_loss: 0.0156 - lr: 1.0000e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history2.history['loss'][1:], label='Training Loss')\n",
        "#plt.plot(history2.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "RgJtA6h8xZPh",
        "outputId": "e4f2c362-c121-45e9-b914-f9dda44cbe0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "do inference to get the data with reduced dimension"
      ],
      "metadata": {
        "id": "CWOUzo_hrZCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Model(input_data, encoded2)\n",
        "compressed_data = encoder.predict(data_noisy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCTBOLO1rT3u",
        "outputId": "1c10e3fd-40d3-4321-9126-be87dcafcd71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 11s 284ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the obtained data"
      ],
      "metadata": {
        "id": "BT0_5WMGrnZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_folder_path = '/content/drive/MyDrive/fSAE/'\n",
        "compressed_folders = [\"comp1\", \"comp2\", \"comp3\", \"comp4\"]\n",
        "samples_per_folder = 300\n",
        "\n",
        "for idx, comp_folder in enumerate(compressed_folders):\n",
        "    # Create the compressed folder if it doesn't exist\n",
        "    comp_folder_path = os.path.join(main_folder_path, comp_folder)\n",
        "    if not os.path.exists(comp_folder_path):\n",
        "        os.makedirs(comp_folder_path)\n",
        "\n",
        "    start_idx = idx * samples_per_folder\n",
        "    end_idx = start_idx + samples_per_folder\n",
        "    print(idx)\n",
        "    for j, encoded_vector in enumerate(compressed_data[start_idx:end_idx]):\n",
        "        file_name = f\"{comp_folder}_{j+1}.mat\"\n",
        "        scipy.io.savemat(os.path.join(comp_folder_path, file_name), {'Y': encoded_vector})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wyOm-w_rpmy",
        "outputId": "c0a633ac-9ae3-4253-e8e6-0d04fc1aee87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    }
  ]
}